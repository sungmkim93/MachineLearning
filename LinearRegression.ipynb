{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machineLearning supervised Learning\n",
    "### Classfication vs Regression\n",
    "Knn, DecisionTree,svn\n",
    "미리 학습을 시켜준곳에 input을 넣으면 output을 보인다\n",
    "<img src=\"Image/LinearRegression_1.png\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "사람의 무게를 줄때, 키를 추측하는것. 1차함수형태 : y = ax+b <br>\n",
    "#### Error - differentce between prediction and real value (실제 있는 점과 선상의 있는 점의 차이)\n",
    "#### Square Error : Error^2\n",
    "장점\n",
    "- 눈에 더 쉽게 보인다. \n",
    "- 에러가 조금이라도 있어도 큰곳과 작은곳을 쉽게 비교할수있다. \n",
    "- 계산이 더 쉽다.\n",
    "- 예측을 조금더 실제값에 가깝게 예측할수있다. \n",
    "\n",
    "### How can you code to find best linear equation?\n",
    "\n",
    "#### Find Linear Equation has Least Mean Square(LMS) Error\n",
    "LMS Error (Least Mean Square Error)\n",
    "<img src=\"Image/LinearRegression_2.png\" style=\"width:600px;height:400px;\">\n",
    "Mean Square Error = how to find least mean square? -> 직선을 긋는 방법? = Gradient Decent!\n",
    "= Square Error 를 평균낸 값.\n",
    "\n",
    "Mean Square Error = Cost Function J(θ): 비용함수(실제, 가설값의 차이) = Objective function\n",
    "\n",
    "#### Gradient Decent(θ 값을 찾기 다음의 θ값을 지금의 θ값으로 찾기) (y=ax 꼴)\n",
    "<img src=\"Image/LinearRegression_3.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_4.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "\n",
    "θ = parameter <br>\n",
    "α = running rate <br>\n",
    "local minimum일 때까지 계속 진행 (θ가 0에 가까워 지도록 step 반복)\n",
    "\n",
    "### Q&A1. What if we have more theta?  (y=ax+b 꼴)\n",
    "h(x) = θ1x + θ0\n",
    "<img src=\"Image/LinearRegression_5.png\" style=\"width:600px;height:400px;\">\n",
    "\n",
    "\n",
    "### Q&A2. How to decide learning rate? 너무 작게 or 너무 높게 잡지 않는다.\n",
    "<img src=\"Image/LinearRegression_6.png\" style=\"width:600px;height:400px;\">\n",
    "\n",
    "너무 작으면? 시간이 너무 오래걸린다. (minimum으로 내려가는데..)      너무 높으면? 최저점으로 수렴하지 않을수도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression's Hypothesis and cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training data 를 통해서 regression model을 만든다\n",
    "<img src=\"Image/LinearRegression_Deep1.png\" style=\"width:600;height:500px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep2.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep3.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "(Linear)Hypothsis: Linear한 형태라고 가정하고 data를 통해 모델을 만드는것\n",
    "data 에 맞는 Linear한 일차함수를 찾는 것 = 학습을 하는 것. H(x) = Wx+b 의 형태 <br>\n",
    "W,b에 값에 따라 다른 형태의 Linear한 형태로 나타날것이다.\n",
    "\n",
    "Cost Function = Loss Function -> 보통 distance 를 계산 할때는 차이를 제곱한다. <br>\n",
    "-> 장점. 일정하게 양수로 표현 가능하다. 차이가 클때 더 값들이 커져서 차이를 작게 만들수있다. \n",
    "\n",
    "Goal : Minimize cost -> minimize cost(W,b) : (주어진 식에서)Weight,bias를 최소화하는 다양한 알고리즘이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation LinearRegression using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build graph using TF operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 19.741056 [2.5211456] [1.223675]\n",
      "20 0.20683064 [0.96952546] [0.5150542]\n",
      "40 0.027163653 [0.8300204] [0.42885017]\n",
      "60 0.0232136 [0.82458895] [0.40279236]\n",
      "80 0.021069752 [0.8315548] [0.38330045]\n",
      "100 0.019135771 [0.8393493] [0.36523327]\n",
      "120 0.017379422 [0.84688777] [0.3480636]\n",
      "140 0.015784264 [0.8540824] [0.3317054]\n",
      "160 0.014335536 [0.86093986] [0.3161164]\n",
      "180 0.013019753 [0.8674753] [0.30126008]\n",
      "200 0.011824737 [0.8737034] [0.2871019]\n",
      "220 0.010739415 [0.8796389] [0.27360916]\n",
      "240 0.009753711 [0.8852954] [0.26075056]\n",
      "260 0.0088584805 [0.89068604] [0.2484963]\n",
      "280 0.00804541 [0.8958235] [0.23681787]\n",
      "300 0.007306974 [0.9007193] [0.22568832]\n",
      "320 0.006636303 [0.90538514] [0.21508183]\n",
      "340 0.006027207 [0.90983164] [0.20497379]\n",
      "360 0.0054740054 [0.9140693] [0.19534075]\n",
      "380 0.004971574 [0.91810775] [0.18616046]\n",
      "400 0.004515266 [0.9219563] [0.1774116]\n",
      "420 0.0041008424 [0.9256241] [0.16907391]\n",
      "440 0.0037244577 [0.92911947] [0.1611281]\n",
      "460 0.0033826053 [0.9324506] [0.15355566]\n",
      "480 0.0030721312 [0.93562514] [0.14633916]\n",
      "500 0.002790166 [0.93865037] [0.13946179]\n",
      "520 0.0025340705 [0.94153374] [0.13290758]\n",
      "540 0.002301492 [0.94428146] [0.12666142]\n",
      "560 0.0020902448 [0.9469] [0.12070877]\n",
      "580 0.0018983932 [0.94939566] [0.11503588]\n",
      "600 0.001724148 [0.9517738] [0.10962956]\n",
      "620 0.0015658997 [0.9540403] [0.10447737]\n",
      "640 0.0014221725 [0.9562002] [0.09956732]\n",
      "660 0.0012916407 [0.9582586] [0.09488802]\n",
      "680 0.0011730887 [0.96022034] [0.09042865]\n",
      "700 0.0010654195 [0.96208984] [0.0861788]\n",
      "720 0.0009676309 [0.9638715] [0.08212871]\n",
      "740 0.0008788163 [0.9655694] [0.07826894]\n",
      "760 0.00079815387 [0.9671875] [0.07459055]\n",
      "780 0.0007248988 [0.96872944] [0.0710851]\n",
      "800 0.00065836584 [0.9701991] [0.06774445]\n",
      "820 0.00059793814 [0.97159964] [0.06456069]\n",
      "840 0.00054305414 [0.97293437] [0.06152659]\n",
      "860 0.00049321586 [0.9742063] [0.05863506]\n",
      "880 0.00044794497 [0.97541857] [0.05587944]\n",
      "900 0.00040682868 [0.97657377] [0.05325332]\n",
      "920 0.00036949015 [0.9776747] [0.05075061]\n",
      "940 0.0003355754 [0.9787238] [0.04836555]\n",
      "960 0.00030477505 [0.97972393] [0.04609256]\n",
      "980 0.00027680254 [0.98067683] [0.04392629]\n",
      "1000 0.00025139688 [0.9815849] [0.04186187]\n",
      "1020 0.00022831994 [0.9824503] [0.03989451]\n",
      "1040 0.00020736433 [0.98327506] [0.03801963]\n",
      "1060 0.00018833154 [0.98406106] [0.03623285]\n",
      "1080 0.00017104576 [0.9848102] [0.03453006]\n",
      "1100 0.00015534768 [0.98552406] [0.03290726]\n",
      "1120 0.00014108927 [0.9862043] [0.03136074]\n",
      "1140 0.00012813804 [0.9868527] [0.0298869]\n",
      "1160 0.00011637809 [0.98747057] [0.02848234]\n",
      "1180 0.000105697865 [0.9880594] [0.02714379]\n",
      "1200 9.59943e-05 [0.9886206] [0.02586812]\n",
      "1220 8.718363e-05 [0.98915535] [0.02465242]\n",
      "1240 7.918278e-05 [0.989665] [0.0234939]\n",
      "1260 7.191454e-05 [0.99015075] [0.02238974]\n",
      "1280 6.531357e-05 [0.9906136] [0.02133748]\n",
      "1300 5.9319165e-05 [0.9910548] [0.02033468]\n",
      "1320 5.3874228e-05 [0.99147516] [0.01937901]\n",
      "1340 4.8929418e-05 [0.99187577] [0.01846825]\n",
      "1360 4.4437955e-05 [0.99225765] [0.0176003]\n",
      "1380 4.035929e-05 [0.9926215] [0.01677315]\n",
      "1400 3.6655656e-05 [0.99296826] [0.01598485]\n",
      "1420 3.32912e-05 [0.9932987] [0.01523362]\n",
      "1440 3.0235336e-05 [0.99361366] [0.01451769]\n",
      "1460 2.7459915e-05 [0.9939138] [0.0138354]\n",
      "1480 2.4939398e-05 [0.9941998] [0.0131852]\n",
      "1500 2.2650702e-05 [0.9944724] [0.01256555]\n",
      "1520 2.0571902e-05 [0.99473214] [0.01197503]\n",
      "1540 1.8683579e-05 [0.9949797] [0.01141228]\n",
      "1560 1.6968876e-05 [0.99521565] [0.01087593]\n",
      "1580 1.5411324e-05 [0.9954405] [0.01036481]\n",
      "1600 1.399679e-05 [0.9956548] [0.00987771]\n",
      "1620 1.2712381e-05 [0.995859] [0.00941346]\n",
      "1640 1.1545543e-05 [0.99605364] [0.00897105]\n",
      "1660 1.04858955e-05 [0.99623907] [0.00854945]\n",
      "1680 9.522979e-06 [0.99641585] [0.00814766]\n",
      "1700 8.648977e-06 [0.99658424] [0.00776475]\n",
      "1720 7.855566e-06 [0.9967448] [0.00739984]\n",
      "1740 7.134218e-06 [0.99689776] [0.00705208]\n",
      "1760 6.479608e-06 [0.9970436] [0.00672066]\n",
      "1780 5.8847595e-06 [0.99718255] [0.00640478]\n",
      "1800 5.3445906e-06 [0.99731493] [0.00610376]\n",
      "1820 4.8541333e-06 [0.99744105] [0.00581693]\n",
      "1840 4.408679e-06 [0.9975614] [0.00554358]\n",
      "1860 4.0038144e-06 [0.99767596] [0.00528305]\n",
      "1880 3.6364445e-06 [0.9977852] [0.00503477]\n",
      "1900 3.3027145e-06 [0.9978893] [0.00479816]\n",
      "1920 2.9995326e-06 [0.99798846] [0.00457266]\n",
      "1940 2.7244207e-06 [0.998083] [0.00435776]\n",
      "1960 2.4740898e-06 [0.99817306] [0.00415299]\n",
      "1980 2.2473555e-06 [0.99825895] [0.00395782]\n",
      "2000 2.0407854e-06 [0.9983407] [0.00377182]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 2.82329 [ 2.12867713] [-0.85235667]\\n20 0.190351 [ 1.53392804] [-1.05059612]\\n40 0.151357 [ 1.45725465] [-1.02391243]\\n...\\n1920 1.77484e-05 [ 1.00489295] [-0.01112291]\\n1940 1.61197e-05 [ 1.00466311] [-0.01060018]\\n1960 1.46397e-05 [ 1.004444] [-0.01010205]\\n1980 1.32962e-05 [ 1.00423515] [-0.00962736]\\n2000 1.20761e-05 [ 1.00403607] [-0.00917497]\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# X and Y data \n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "# Variable: Tensorflow가 사용하는 Var이다. Tensorflow가 실행되면서 자체적으로 변경되는 값이다. (Trainable)\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis XW+b 가설\n",
    "hypothesis = x_train * W + b\n",
    "#-----------------------------------------------------------------------------------\n",
    "# cost function / loss function 구현\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# Minimize 그래프 구현 \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session. 실행 \n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line sess를 통해 train Node를 실행을 시킨다. (2000번)을 20번에 1번씩 실행\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0.] 가 되어야 한다. \n",
    "\n",
    "'''\n",
    "0 2.82329 [ 2.12867713] [-0.85235667]\n",
    "20 0.190351 [ 1.53392804] [-1.05059612]\n",
    "40 0.151357 [ 1.45725465] [-1.02391243]\n",
    "...\n",
    "1920 1.77484e-05 [ 1.00489295] [-0.01112291]\n",
    "1940 1.61197e-05 [ 1.00466311] [-0.01060018]\n",
    "1960 1.46397e-05 [ 1.004444] [-0.01010205]\n",
    "1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n",
    "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.9983544 [2.1911864] [-0.23809919]\n",
      "20 0.11629844 [1.3543705] [-0.5703999]\n",
      "40 0.04905046 [1.263366] [-0.5763002]\n",
      "60 0.044035435 [1.243909] [-0.55233043]\n",
      "80 0.039989036 [1.2317722] [-0.5266694]\n",
      "100 0.036318634 [1.2208154] [-0.50194603]\n",
      "120 0.032985162 [1.2104318] [-0.47835913]\n",
      "140 0.029957622 [1.2005416] [-0.45587823]\n",
      "160 0.02720801 [1.1911169] [-0.43445364]\n",
      "180 0.024710773 [1.1821351] [-0.41403598]\n",
      "200 0.02244273 [1.1735754] [-0.39457783]\n",
      "220 0.02038282 [1.165418] [-0.37603408]\n",
      "240 0.018512012 [1.1576439] [-0.3583618]\n",
      "260 0.016812906 [1.1502353] [-0.3415202]\n",
      "280 0.015269752 [1.143175] [-0.32546997]\n",
      "300 0.01386822 [1.1364461] [-0.31017405]\n",
      "320 0.012595359 [1.1300336] [-0.29559702]\n",
      "340 0.011439276 [1.1239226] [-0.281705]\n",
      "360 0.010389345 [1.1180986] [-0.26846594]\n",
      "380 0.009435758 [1.1125484] [-0.25584906]\n",
      "400 0.008569714 [1.107259] [-0.24382503]\n",
      "420 0.0077831685 [1.1022185] [-0.23236619]\n",
      "440 0.0070687905 [1.0974144] [-0.22144583]\n",
      "460 0.0064199935 [1.0928363] [-0.21103868]\n",
      "480 0.005830724 [1.0884733] [-0.20112059]\n",
      "500 0.0052955695 [1.0843154] [-0.19166867]\n",
      "520 0.0048095235 [1.0803529] [-0.18266098]\n",
      "540 0.004368072 [1.0765766] [-0.17407657]\n",
      "560 0.003967168 [1.0729779] [-0.16589566]\n",
      "580 0.0036030405 [1.069548] [-0.15809914]\n",
      "600 0.003272336 [1.0662795] [-0.15066905]\n",
      "620 0.0029719889 [1.0631647] [-0.14358814]\n",
      "640 0.0026992026 [1.0601959] [-0.13683997]\n",
      "660 0.0024514569 [1.0573671] [-0.13040891]\n",
      "680 0.0022264465 [1.054671] [-0.12428012]\n",
      "700 0.0020220994 [1.0521017] [-0.11843942]\n",
      "720 0.0018365029 [1.049653] [-0.11287316]\n",
      "740 0.0016679425 [1.0473197] [-0.10756855]\n",
      "760 0.0015148496 [1.0450957] [-0.10251325]\n",
      "780 0.001375815 [1.0429765] [-0.09769554]\n",
      "800 0.001249536 [1.0409569] [-0.09310431]\n",
      "820 0.0011348521 [1.0390319] [-0.08872879]\n",
      "840 0.0010306862 [1.0371975] [-0.08455883]\n",
      "860 0.0009360865 [1.0354494] [-0.08058482]\n",
      "880 0.00085016736 [1.0337833] [-0.0767976]\n",
      "900 0.0007721367 [1.0321957] [-0.07318839]\n",
      "920 0.00070126896 [1.0306827] [-0.06974883]\n",
      "940 0.0006369031 [1.0292407] [-0.06647092]\n",
      "960 0.00057844556 [1.0278665] [-0.06334706]\n",
      "980 0.00052535464 [1.0265568] [-0.06036996]\n",
      "1000 0.00047713742 [1.0253087] [-0.05753278]\n",
      "1020 0.0004333403 [1.0241194] [-0.05482899]\n",
      "1040 0.00039356833 [1.0229858] [-0.05225226]\n",
      "1060 0.00035744836 [1.0219057] [-0.04979663]\n",
      "1080 0.00032464138 [1.0208764] [-0.04745645]\n",
      "1100 0.00029484477 [1.0198953] [-0.04522631]\n",
      "1120 0.00026777873 [1.01896] [-0.04310082]\n",
      "1140 0.00024320187 [1.018069] [-0.0410752]\n",
      "1160 0.00022087962 [1.0172198] [-0.03914476]\n",
      "1180 0.00020060783 [1.0164105] [-0.03730508]\n",
      "1200 0.00018219471 [1.0156393] [-0.03555182]\n",
      "1220 0.00016547092 [1.0149044] [-0.03388102]\n",
      "1240 0.00015028403 [1.0142038] [-0.03228872]\n",
      "1260 0.00013648962 [1.0135362] [-0.03077124]\n",
      "1280 0.0001239601 [1.0129001] [-0.02932506]\n",
      "1300 0.000112585294 [1.0122938] [-0.0279469]\n",
      "1320 0.00010225044 [1.0117161] [-0.0266335]\n",
      "1340 9.2865645e-05 [1.0111655] [-0.02538183]\n",
      "1360 8.434284e-05 [1.0106407] [-0.02418897]\n",
      "1380 7.6599936e-05 [1.0101407] [-0.02305218]\n",
      "1400 6.956899e-05 [1.009664] [-0.02196877]\n",
      "1420 6.318401e-05 [1.0092099] [-0.02093631]\n",
      "1440 5.7385638e-05 [1.0087771] [-0.01995238]\n",
      "1460 5.2117848e-05 [1.0083646] [-0.0190147]\n",
      "1480 4.733367e-05 [1.0079715] [-0.01812109]\n",
      "1500 4.2990596e-05 [1.0075969] [-0.01726948]\n",
      "1520 3.9043895e-05 [1.0072399] [-0.0164579]\n",
      "1540 3.5460984e-05 [1.0068997] [-0.01568445]\n",
      "1560 3.2205888e-05 [1.0065753] [-0.01494737]\n",
      "1580 2.9250188e-05 [1.0062662] [-0.01424486]\n",
      "1600 2.65656e-05 [1.0059718] [-0.01357538]\n",
      "1620 2.41269e-05 [1.0056912] [-0.01293737]\n",
      "1640 2.191255e-05 [1.0054238] [-0.0123294]\n",
      "1660 1.9901361e-05 [1.0051689] [-0.01174999]\n",
      "1680 1.8075272e-05 [1.0049258] [-0.01119781]\n",
      "1700 1.6415363e-05 [1.0046945] [-0.01067155]\n",
      "1720 1.49088855e-05 [1.0044738] [-0.01017002]\n",
      "1740 1.35406035e-05 [1.0042636] [-0.00969209]\n",
      "1760 1.2297577e-05 [1.0040632] [-0.00923661]\n",
      "1780 1.1169741e-05 [1.0038723] [-0.00880252]\n",
      "1800 1.0143852e-05 [1.0036902] [-0.00838883]\n",
      "1820 9.213065e-06 [1.0035169] [-0.0079946]\n",
      "1840 8.367313e-06 [1.0033516] [-0.0076189]\n",
      "1860 7.599245e-06 [1.003194] [-0.00726085]\n",
      "1880 6.9020157e-06 [1.0030439] [-0.00691963]\n",
      "1900 6.2685504e-06 [1.002901] [-0.00659446]\n",
      "1920 5.692993e-06 [1.0027646] [-0.00628455]\n",
      "1940 5.1705533e-06 [1.0026348] [-0.0059892]\n",
      "1960 4.696141e-06 [1.0025109] [-0.00570777]\n",
      "1980 4.2654315e-06 [1.0023929] [-0.00543959]\n",
      "2000 3.873649e-06 [1.0022805] [-0.00518396]\n",
      "[5.006218]\n",
      "[2.5005171]\n",
      "[1.4982368 3.5027978]\n",
      "0 1.2063668 [1.0680898] [0.01678289]\n",
      "20 0.16768473 [1.2639668] [0.14321357]\n",
      "40 0.14642224 [1.2475843] [0.20612496]\n",
      "60 0.1278716 [1.2313739] [0.26466668]\n",
      "80 0.11167135 [1.2162211] [0.31937328]\n",
      "100 0.09752338 [1.2020606] [0.3704971]\n",
      "120 0.08516788 [1.1888274] [0.41827285]\n",
      "140 0.07437779 [1.1764609] [0.46291974]\n",
      "160 0.06495466 [1.1649044] [0.5046426]\n",
      "180 0.05672537 [1.1541047] [0.5436331]\n",
      "200 0.049538683 [1.1440123] [0.5800699]\n",
      "220 0.043262485 [1.1345809] [0.61412054]\n",
      "240 0.037781466 [1.1257671] [0.64594114]\n",
      "260 0.032994874 [1.1175305] [0.6756777]\n",
      "280 0.028814655 [1.1098332] [0.70346683]\n",
      "300 0.025164044 [1.10264] [0.7294362]\n",
      "320 0.021975953 [1.0959182] [0.75370485]\n",
      "340 0.01919176 [1.0896363] [0.776384]\n",
      "360 0.016760282 [1.083766] [0.79757786]\n",
      "380 0.014636862 [1.0782801] [0.8173838]\n",
      "400 0.012782484 [1.0731535] [0.8358926]\n",
      "420 0.011163034 [1.0683626] [0.8531893]\n",
      "440 0.009748762 [1.0638855] [0.8693531]\n",
      "460 0.00851367 [1.0597014] [0.88445836]\n",
      "480 0.0074350396 [1.0557916] [0.89857435]\n",
      "500 0.0064930925 [1.0521379] [0.91176575]\n",
      "520 0.00567047 [1.0487232] [0.9240934]\n",
      "540 0.00495205 [1.0455322] [0.93561375]\n",
      "560 0.0043246495 [1.0425503] [0.94637966]\n",
      "580 0.00377675 [1.0397636] [0.9564405]\n",
      "600 0.0032982558 [1.0371594] [0.9658424]\n",
      "620 0.0028803896 [1.0347259] [0.97462845]\n",
      "640 0.0025154676 [1.0324517] [0.98283905]\n",
      "660 0.0021967725 [1.0303262] [0.990512]\n",
      "680 0.0019184652 [1.0283402] [0.9976826]\n",
      "700 0.0016754037 [1.0264843] [1.0043834]\n",
      "720 0.0014631503 [1.0247498] [1.0106454]\n",
      "740 0.0012777764 [1.023129] [1.0164973]\n",
      "760 0.0011158937 [1.0216143] [1.0219659]\n",
      "780 0.0009745229 [1.0201986] [1.0270762]\n",
      "800 0.0008510621 [1.0188758] [1.031852]\n",
      "820 0.0007432376 [1.0176398] [1.036315]\n",
      "840 0.0006490762 [1.0164845] [1.0404857]\n",
      "860 0.00056684273 [1.0154049] [1.0443834]\n",
      "880 0.00049503223 [1.014396] [1.0480256]\n",
      "900 0.000432315 [1.0134532] [1.0514294]\n",
      "920 0.00037754062 [1.0125722] [1.0546104]\n",
      "940 0.00032971223 [1.0117489] [1.0575829]\n",
      "960 0.00028794075 [1.0109794] [1.0603609]\n",
      "980 0.00025146152 [1.0102605] [1.0629567]\n",
      "1000 0.00021960582 [1.0095884] [1.0653825]\n",
      "1020 0.00019177972 [1.0089604] [1.0676498]\n",
      "1040 0.00016748505 [1.0083736] [1.0697683]\n",
      "1060 0.00014626232 [1.0078251] [1.0717485]\n",
      "1080 0.00012773216 [1.0073128] [1.0735986]\n",
      "1100 0.000111552086 [1.0068338] [1.0753275]\n",
      "1120 9.741963e-05 [1.0063863] [1.0769433]\n",
      "1140 8.5077416e-05 [1.0059681] [1.0784532]\n",
      "1160 7.429699e-05 [1.0055772] [1.0798646]\n",
      "1180 6.488683e-05 [1.005212] [1.0811831]\n",
      "1200 5.666518e-05 [1.0048705] [1.0824153]\n",
      "1220 4.948749e-05 [1.0045516] [1.0835669]\n",
      "1240 4.3215005e-05 [1.0042535] [1.0846432]\n",
      "1260 3.7740472e-05 [1.003975] [1.085649]\n",
      "1280 3.2960415e-05 [1.0037147] [1.0865889]\n",
      "1300 2.878458e-05 [1.0034715] [1.0874671]\n",
      "1320 2.5136716e-05 [1.003244] [1.0882877]\n",
      "1340 2.1952814e-05 [1.0030316] [1.0890548]\n",
      "1360 1.9172863e-05 [1.0028331] [1.0897713]\n",
      "1380 1.6744136e-05 [1.0026476] [1.0904411]\n",
      "1400 1.4622335e-05 [1.0024743] [1.0910671]\n",
      "1420 1.277126e-05 [1.0023122] [1.091652]\n",
      "1440 1.1152916e-05 [1.0021608] [1.0921987]\n",
      "1460 9.739294e-06 [1.0020193] [1.0927095]\n",
      "1480 8.505734e-06 [1.0018871] [1.0931872]\n",
      "1500 7.4276163e-06 [1.0017635] [1.0936334]\n",
      "1520 6.486944e-06 [1.001648] [1.0940503]\n",
      "1540 5.6650547e-06 [1.0015401] [1.0944399]\n",
      "1560 4.9471473e-06 [1.0014392] [1.0948039]\n",
      "1580 4.321054e-06 [1.001345] [1.095144]\n",
      "1600 3.7737314e-06 [1.001257] [1.095462]\n",
      "1620 3.295662e-06 [1.0011747] [1.0957593]\n",
      "1640 2.8779777e-06 [1.0010978] [1.0960369]\n",
      "1660 2.5134118e-06 [1.0010259] [1.0962964]\n",
      "1680 2.194836e-06 [1.0009587] [1.0965389]\n",
      "1700 1.9170443e-06 [1.0008959] [1.0967656]\n",
      "1720 1.6742491e-06 [1.0008373] [1.0969772]\n",
      "1740 1.4621692e-06 [1.0007825] [1.0971751]\n",
      "1760 1.2770809e-06 [1.0007312] [1.09736]\n",
      "1780 1.1152263e-06 [1.0006833] [1.0975329]\n",
      "1800 9.741976e-07 [1.0006387] [1.0976944]\n",
      "1820 8.507932e-07 [1.0005969] [1.0978452]\n",
      "1840 7.432317e-07 [1.0005579] [1.0979861]\n",
      "1860 6.4901064e-07 [1.0005213] [1.0981181]\n",
      "1880 5.6690516e-07 [1.0004872] [1.0982411]\n",
      "1900 4.950001e-07 [1.0004554] [1.0983564]\n",
      "1920 4.3236258e-07 [1.0004256] [1.098464]\n",
      "1940 3.7763272e-07 [1.0003977] [1.0985644]\n",
      "1960 3.2982302e-07 [1.0003716] [1.0986583]\n",
      "1980 2.8802492e-07 [1.0003474] [1.0987462]\n",
      "2000 2.5156558e-07 [1.0003246] [1.0988282]\n",
      "[6.1004515]\n",
      "[3.59964]\n",
      "[2.5993152 4.599964 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1960 3.32396e-07 [ 1.00037301] [ 1.09865296]\\n1980 2.90429e-07 [ 1.00034881] [ 1.09874094]\\n2000 2.5373e-07 [ 1.00032604] [ 1.09882331]\\n[ 6.10045338]\\n[ 3.59963846]\\n[ 2.59931231  4.59996414]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let's use TensorFlow to figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Now we can use X and Y in place of x_data and y_data\n",
    "# # placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "# 실제 값을 직접 주지않고 x,y값을 placeholder라고 줄수있다. \n",
    "X = tf.placeholder(tf.float32, shape=[None]) \n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "        sess.run([cost, W, b, train],\n",
    "                 feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0]\n",
    "'''\n",
    "...\n",
    "1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n",
    "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n",
    "'''\n",
    "\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n",
    "\n",
    "'''\n",
    "[ 5.0110054]\n",
    "[ 2.50091505]\n",
    "[ 1.49687922  3.50495124]\n",
    "'''\n",
    "\n",
    "\n",
    "# Fit the line with new training data (train이라는 노드를 생성할때, 값을 넘겨줄수있다.)\n",
    "# train 시킬때, 직접값을 주지않고 feed_dict로 넘겨 줄수있다. (placeholder 이용)\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "        sess.run([cost, W, b, train],\n",
    "                 feed_dict={X: [1, 2, 3, 4, 5],\n",
    "                            Y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, W_val, b_val)\n",
    "\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n",
    "\n",
    "'''\n",
    "1960 3.32396e-07 [ 1.00037301] [ 1.09865296]\n",
    "1980 2.90429e-07 [ 1.00034881] [ 1.09874094]\n",
    "2000 2.5373e-07 [ 1.00032604] [ 1.09882331]\n",
    "[ 6.10045338]\n",
    "[ 3.59963846]\n",
    "[ 2.59931231  4.59996414]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 동작 그림으로 이해\n",
    "\n",
    "<img src=\"Image/LinearRegression_Deep4.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep5.png\" style=\"width:350px;height:250px;display:inline-block;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression Cost minimizing Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표: cost 를 minimize 하는 W,b를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simplified hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개념\n",
    "<img src=\"Image/LinearRegression_Deep6.png\" style=\"width:600px;height:400px;\">\n",
    "bias가 없는 간단한 형태로만 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent algorithm <br>\n",
    "어떤 점에서 시작하던지 항상 최솟값을 찾을수있다. = 미분이용!<br>\n",
    "<img src=\"Image/LinearRegression_Deep7.png\" style=\"width:350px;height:250px;\">\n",
    "<img src=\"Image/LinearRegression_Deep8.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep9.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep10.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep11.png\" style=\"width:350px;height:250px;display:inline-block;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Image/LinearRegression_Deep12.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "<img src=\"Image/LinearRegression_Deep13.png\" style=\"width:350px;height:250px;display:inline-block;\">\n",
    "오른쪽 그림과 같은 모양이면 어떻게든 한점으로 수렴하기때문에 걱정없이 알고리즘을 수행해도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181cd61518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_history = []\n",
    "cost_history = []\n",
    "\n",
    "for i in range(-30, 50):\n",
    "    curr_W = i * 0.1\n",
    "    curr_cost = sess.run(cost, feed_dict={W: curr_W})\n",
    "    W_history.append(curr_W)\n",
    "    cost_history.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_history, cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
